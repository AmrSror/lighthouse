patch-package
--- a/node_modules/robots-parser/Robots.js
+++ b/node_modules/robots-parser/Robots.js
@@ -1,4 +1,4 @@
-var libUrl   = require('url');
+var URL   = (typeof self !== 'undefined' && self.URL) || require('url').URL;
+// Add Fallback for Node 6, which doesn't have require('url').URL
+var getUrlPath;
+var parse;
+if (typeof URL === 'undefined') {
+  parse = require('url').parse;
+  getUrlPath = parsedUrl => parsedUrl.path;
+} else {
+  parse = url => new URL(url);
+  getUrlPath = parsedUrl => parsedUrl.pathname + parsedUrl.search;
+}
 var punycode = require('punycode');
 
 /**
@@ -176,7 +176,7 @@ function isPathAllowed(path, rules) {
 
 
 function Robots(url, contents) {
-	this._url = libUrl.parse(url);
+	this._url = parse(url);
 	this._url.port = this._url.port || 80;
 	this._url.hostname = punycode.toUnicode(this._url.hostname);
 
@@ -262,7 +262,7 @@ Robots.prototype.setPreferredHost = function (url) {
  * @return {boolean?}
  */
 Robots.prototype.isAllowed = function (url, ua) {
-	var parsedUrl = libUrl.parse(url);
+	this._url = parse(url);
 	var userAgent = formatUserAgent(ua || '*');
 
 	parsedUrl.port = parsedUrl.port || 80;
@@ -277,7 +277,7 @@ Robots.prototype.isAllowed = function (url, ua) {
 
 	var rules = this._rules[userAgent] || this._rules['*'] || [];
 
-	return isPathAllowed(parsedUrl.path, rules);
+	return isPathAllowed(getPath(parsedUrl), rules);
 };
 
 /**
